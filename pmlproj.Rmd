---
title: "PML Project"
author: "Bob Waters"
date: "Friday, April 29, 2016"
output:
  html_document:
    keep_md: yes
---
ABSTRACT

A random forest procedure for determining how well a certain exercise is performed by identifying the proper motion and four common mistakes of the weight lifting exercise is described. Accelerometer data from three body positions and one on a dumbell were used to build the model.  These data are found in the Weight Lifting Exercises Dataset of the Human Activity (HAR) Recognition project, and can be accessed at http://groupware.les.inf.puc-rio.br/har.  The random forest model was used to accurately predict twenty unknown motions from a test dataset.

MODEL BUILDING

Required R Packages

The following R packages are required, and the training data loaded.

```{r}
options(warn = -1)
library(ggplot2)
library(dplyr)
library(caret)
library(doParallel)
set.seed(17)

pml.training <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))

```

Factor Selection

The training dataset was cleaned so that the resulting dataset contains only data and these data are in common with the test dataset.  The resulting clean dataset contains 53 columns, 52 factors and and 1 outcome, "classe."   This dataset was the subdivided into training data set to be used for model building and a test dataset for validation purposes.  Due to computer limitations, the dataset was split evenly (p = 0.5). 

```{r}
cln.training<-select(pml.training,8:11,37:49,60:68,84:86,102,113:124,140,151:160)

inTrain<-createDataPartition(y=cln.training$classe,p=0.5,list=FALSE)
train<-cln.training[inTrain,]
test<-cln.training[-inTrain,]
```

Model 1

The first model developed uses all 52 factors to predict the outcome.  K-fold cross validation was used for error estimation of the training data.  A K of 5 was used to split the training data into about 7850 samples for error estimation against the remaining data used for model training.

```{r}

fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)

modFit<-train(classe~.,
              data=train, 
              method="rf",
              trControl= fitControl,
              prox=TRUE)

modFit

```

The resulting model gave a high resubstitution accuracy of 98.7% from the 5 fold cross validation.  The variable importance was determined for this model.

```{r}
varImp(modFit)
```

The variable importance is shown graphically below.

```{r, echo=TRUE}

varImpPlot(modFit$finalModel)

```

The model depends mostly on seven variable that are distinctly more important than the remaining factors.  These important factors as determined by the Gini index are: roll_belt, pitch_forearm, yaw_belt, magnet_dumbbell_z, pitch_belt, magnet_dumbell_y, and roll_forearm.

Model 2

A second random forest model was built that used only the seven most important factors.  Again, the training dataset was divided evenly with the same random seed to ensure the same data split selection.  Model 2 was trained using 5 fold cross validation.

```{r}
run.training<-select(pml.training,8:10,120:123,160)
inTrain.run<-createDataPartition(y=run.training$classe,p=0.50,list=FALSE)
train.run<-run.training[inTrain.run,]
test.run<-run.training[-inTrain.run,]

modRun<-train(classe~.,
              data=train.run, 
              method="rf",
              trControl=fitControl,
              prox=TRUE)
modRun
```

Model 2 performed nearly as well as Model 1 with modestly lower accuracy of 97.5 compared to 98.7.

The confusion matrices of the models were compard.  First the 52 factor Model 1.

```{r}
cm.mf<-confusionMatrix(modFit)
cm.mf
```

And Model 2.

```{r}
cm.mr<-confusionMatrix(modRun)
cm.mr
```

The confusion matrices of the training data are similar with the 7 factor model showing the expected lower values.  

To get a better sense of model accuracy, the hold out test data from the splits were used to make predictions.  First Model 1 was run.

```{r}
testFit<-predict(modFit,test)
cm.tf<-confusionMatrix(testFit,test$classe)
cm.tf
```

Predictions on the subdivided test set by Model 1.

```{r}
testRun<-predict(modRun,test.run)
cm.tr<-confusionMatrix(testRun,test.run$classe)
cm.tr
```

As seen with the training data, the confusion matrices of the training data are similar with the 7 factor model showing the expected lower values given that the test set accuracy is 97.8% compared to 99.1% for the 52 factor model. 

TEST PREDICTIONS

Both models were that used to predict the twenty sample test set supplied for the course quiz.  The results for these predictions are below.

```{r}
pml.testing <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

data.tf<-select(pml.testing,8:11,37:49,60:68,84:86,102,113:124,140,151:159)
data.tr<-select(pml.testing,8:10,120:123)

pred.tf<-predict(modFit,data.tf)
pred.tr<-predict(modRun,data.tr)

results<-cbind(as.character(pred.tf),as.character(pred.tr))
colnames(results)<-c("Model.1","Model.2")
results

```

The methods gave identical predictions.  For practical considerations, Model 2 is less complicated, and therefore less likely to overfit.  However, it would seem from the results of both methods that overfitting is not a problem.  There was no noticable reduction in processing time between the two methods, but since Model 1 has a slightly higher accuracy it is probably the best of the two methods developed.

